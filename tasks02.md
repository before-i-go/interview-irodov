# TDD Plan for archive-to-txt Crate

## Phase 1: Project Setup (Completed) âœ…

- [x] Initialize project structure
- [x] Set up Cargo.toml with dependencies
- [x] Create basic project layout
- [x] Set up CI/CD pipeline
- [x] Configure code quality tools (clippy, rustfmt)

## Phase 2: Core Test Cases (In Progress) ðŸš§

### Parallel Processing Implementation (Completed) âœ…
- [x] Implemented thread-safe parallel processing using Rayon
- [x] Added configuration option for parallel/sequential processing
- [x] Implemented atomic counter for thread-safe file counting
- [x] Added comprehensive test case for parallel processing
- [x] Ensured proper error handling in parallel execution context

Key Implementation Details:
- Uses `Arc<Mutex<...>>` for thread-safe writer access
- Implements `Send + Sync` for thread-safe formatters
- Maintains consistent output format between parallel/sequential modes

### Test Implementation Status
- [x] Basic empty directory test âœ“
- [x] Single file content test âœ“
- [x] Parallel processing test (using Rayon) âœ“
- [x] Basic error handling âœ“

### Test Cases

### Test 1: Empty Directory (Implemented & Passing)
```rust
#[rstest]
fn test_empty_directory() -> TestResult {
    let temp_dir = assert_fs::TempDir::new()?;
    let output_file = temp_dir.path().join("archive.txt");
    
    let config = Config::default()
        .with_input(temp_dir.path())
        .with_output(&output_file);
        
    let result = archive_directory(temp_dir.path(), &output_file, &config);
    assert!(result.is_ok());
    
    let content = std::fs::read_to_string(&output_file)?;
    assert!(content.contains("Archive created at"));
    assert!(content.contains("Total files processed: 0"));
    
    Ok(())
}
```

### Test 2: Single File (Implemented & Passing)
```rust
#[rstest]
fn test_single_file() -> TestResult {
    let temp_dir = assert_fs::TempDir::new()?;
    let file = temp_dir.child("test.txt");
    file.write_str("Hello, world!")?;
    
    let output_file = temp_dir.path().join("archive.txt");
    let config = Config::default()
        .with_input(temp_dir.path())
        .with_output(&output_file);
    
    let result = archive_directory(temp_dir.path(), &output_file, &config);
    assert!(result.is_ok());
    
    let content = std::fs::read_to_string(&output_file)?;
    assert!(content.contains("test.txt"));
    assert!(content.contains("Hello, world!"));
    
    Ok(())
}
```

### Test 3: Parallel Processing (Planned)
```rust
#[rstest]
fn test_parallel_processing() -> TestResult {
    let temp_dir = assert_fs::TempDir::new()?;
    
    // Create multiple test files
    for i in 0..10 {
        let file = temp_dir.child(format!("test_{}.txt", i));
        file.write_str(&format!("Content {}", i))?;
    }
    
    let output_file = temp_dir.path().join("archive.txt");
    let config = Config::default()
        .with_input(temp_dir.path())
        .with_output(&output_file)
        .with_parallel(true);
    
    let result = archive_directory(temp_dir.path(), &output_file, &config);
    assert!(result.is_ok());
    
    let content = std::fs::read_to_string(&output_file)?;
    for i in 0..10 {
        assert!(content.contains(&format!("test_{}.txt", i)));
        assert!(content.contains(&format!("Content {}", i)));
    }
    
    Ok(())
}
```

### Test 4: File Size Limits (Planned)
```rust
#[rstest]
fn test_file_size_limit() -> TestResult {
    let temp_dir = assert_fs::TempDir::new()?;
    
    // Create a file larger than our limit
    let large_file = temp_dir.child("large.bin");
    let data = vec![0u8; 1024 * 1024]; // 1MB
    large_file.write_binary(&data)?;
    
    let output_file = temp_dir.path().join("archive.txt");
    let config = Config::default()
        .with_input(temp_dir.path())
        .with_output(&output_file)
        .with_max_file_size(Some(512 * 1024)); // 512KB limit
    
    let result = archive_directory(temp_dir.path(), &output_file, &config);
    assert!(matches!(result, Err(ArchiveError::FileTooLarge(_))));
    
    Ok(())
}
```

### Next Steps:
1. Add parallel processing with Rayon
2. Implement file size limits
3. Add binary file detection and handling
4. Add more output formats (JSON, Markdown)
5. Add command-line interface

## Phase 3: Implementation Status

### Core Features
- [x] Basic file walking
- [x] Text file handling
- [x] Basic error handling
- [x] Configuration system
- [x] Text output formatting
- [ ] Parallel processing with Rayon
- [ ] Git integration
- [ ] Binary file handling
- [ ] Performance optimizations
- [ ] Comprehensive documentation

### Output Formats
- [x] Plain text
- [ ] JSON
- [ ] Markdown
- [ ] Custom templates

### Command Line Interface
- [ ] Basic CLI arguments
- [ ] Config file support
- [ ] Progress reporting
- [ ] Verbose/debug mode
- [ ] Custom output formatting

## Phase 4: Next Steps

### High Priority
1. Implement parallel processing with Rayon
2. Add file size limits and proper error handling
3. Implement binary file detection and handling
4. Add JSON output format
5. Create basic command-line interface

### Medium Priority
1. Add Markdown output format
2. Implement Git integration
3. Add progress reporting
4. Add more configuration options
5. Improve error messages and logging

### Future Enhancements
1. Support for custom output templates
2. Archive compression options
3. Remote storage integration
4. Interactive mode
5. Performance benchmarking suite

## MVP Due Diligence Checklist âœ…

### Core Functionality
- [x] Basic directory traversal
- [x] File content extraction
- [x] Parallel processing
- [x] Error handling
- [x] Output formatting

### Code Quality
- [ ] Code documentation (rustdoc)
- [ ] API documentation
- [ ] Error messages and logging
- [ ] Unit test coverage
- [ ] Integration tests

### Build & Release
- [ ] Version number update
- [ ] CHANGELOG.md update
- [ ] Documentation updates
- [ ] Crate metadata
- [ ] License and attribution

### Verification
- [ ] Test on different platforms
- [ ] Test with various file types
- [ ] Test with large directories
- [ ] Memory usage verification
- [ ] Performance validation

## Implementation Details

### Parallel Processing Architecture

1. **Thread Management**:
   - Uses Rayon's work-stealing thread pool
   - Automatically scales with available CPU cores
   - Efficiently balances load across threads

2. **Thread Safety**:
   - Shared writer protected by `Arc<Mutex<...>>`
   - Atomic counters for progress tracking
   - Immutable data sharing where possible

3. **Performance Considerations**:
   - Minimizes lock contention through buffered writes
   - Efficient file path handling
   - Memory-efficient processing of large files

### Core Types
```rust
use std::path::Path;
use std::fs::File;
use std::io::{BufWriter, Write};
use walkdir::WalkDir;
use thiserror::Error;

#[derive(Debug, Clone)]
pub struct Config {
    pub input: std::path::PathBuf,
    pub output: std::path::PathBuf,
    pub include_hidden: bool,
    pub max_file_size: Option<u64>,
    pub parallel: bool,
}
    pub output_format: OutputFormat,
}

impl Default for ArchiveOptions {
    fn default() -> Self {
        Self {
            include_hidden: false,
            max_file_size: None,
            include_gitignore: true,
            parallel: true,
            output_format: OutputFormat::Text,
        }
    }
}

#[derive(Debug, Clone, Copy, PartialEq)]
pub enum OutputFormat {
    Text,
    Json,
    Markdown,
}

#[derive(Debug, thiserror::Error)]
pub enum ArchiveError {
    #[error("I/O error: {0}")]
    Io(#[from] std::io::Error),
    
    #[error("File too large: {0} (max: {1} bytes)")]
    FileTooLarge(String, u64),
    
    #[error("Invalid path: {0}")]
    InvalidPath(String),
    
    #[error("Gitignore error: {0}")]
    GitignoreError(String),
    
    #[error("Serialization error: {0}")]
    Serialization(#[from] serde_json::Error),
}

pub type Result<T> = std::result::Result<T, ArchiveError>;
```

### Main Function with Parallel Processing
```rust
pub fn archive_directory<P: AsRef<Path>>(
    dir: P,
    output_file: &Path,
    options: &ArchiveOptions,
) -> Result<()> {
    let dir = dir.as_ref().canonicalize()
        .map_err(|e| ArchiveError::InvalidPath(format!("Invalid directory: {}", e)))?;
        
    if !dir.is_dir() {
        return Err(ArchiveError::InvalidPath(format!(
            "Not a directory: {}",
            dir.display()
        )));
    }

    // Initialize gitignore if needed
    let gitignore = if options.include_gitignore {
        let mut builder = GitignoreBuilder::new(dir.clone());
        builder.add(".gitignore");
        Some(builder.build()
            .map_err(|e| ArchiveError::GitignoreError(e.to_string()))?)
    } else {
        None
    };

    // Collect all files first for parallel processing
    let entries: Vec<_> = WalkDir::new(&dir)
        .into_iter()
        .filter_entry(|e| {
            let name = e.file_name().to_string_lossy();
            let is_hidden = name.starts_with('.');
            let include = options.include_hidden || !is_hidden;
            
            // Check against gitignore if enabled
            if include && options.include_gitignore {
                if let Some(ref gi) = gitignore {
                    return !gi.matched_path_or_any_parents(e.path(), e.file_type().is_dir())
                        .is_ignore();
                }
            }
            include
        })
        .filter_map(Result::ok)
        .filter(|e| e.file_type().is_file())
        .collect();

    // Process files in parallel or sequentially
    let results: Vec<_> = if options.parallel {
        entries.par_iter()
            .map(|entry| process_entry(entry.path(), &dir, options))
            .collect()
    } else {
        entries.iter()
            .map(|entry| process_entry(entry.path(), &dir, options))
            .collect()
    };

    // Write output
    let mut output = File::create(output_file)?;
    let mut writer = BufWriter::new(&mut output);
    
    // Write header
    writeln!(
        &mut writer,
        "Archive created at {}\n",
        chrono::Local::now().to_rfc3339()
    )?;

    // Write file contents
    let mut file_count = 0;
    for result in results {
        let (path, content) = result?;
        writeln!(&mut writer, "\n=== File: {} ===\n{}", path.display(), content)?;
        file_count += 1;
    }

    // Write footer
    writeln!(&mut writer, "\n=== Summary ===");
    writeln!(&mut writer, "Total files processed: {}", file_count)?;
    writeln!(&mut writer, "Total size: {}", bytesize::to_string(
        std::fs::metadata(output_file)?.len(),
        true
    ))?;

    Ok(())
}

fn process_entry(
    path: &Path,
    base_dir: &Path,
    options: &ArchiveOptions
) -> Result<(PathBuf, String)> {
    // Check file size
    let metadata = std::fs::metadata(path)?;
    if let Some(max_size) = options.max_file_size {
        if metadata.len() > max_size {
            return Err(ArchiveError::FileTooLarge(
                path.display().to_string(),
                max_size
            ));
        }
    }

    // Read file content
    let content = std::fs::read_to_string(path)
        .or_else(|_| {
            // Fallback to binary if not valid UTF-8
            let bytes = std::fs::read(path)?;
            Ok(format!("[Binary content: {} bytes]", bytes.len()))
        })?;

    // Get relative path
    let rel_path = path.strip_prefix(base_dir)
        .unwrap_or(path)
        .to_path_buf();

    Ok((rel_path, content))
}
```

## Phase 4: Verification & Refinement

### Verification Checklist
- [ ] Test with large directories
- [ ] Verify error handling
- [ ] Check memory usage
- [ ] Test cross-platform compatibility
- [ ] Performance benchmarking
- [ ] Documentation review
- [ ] API stability check
- [ ] Security audit

### Future Extensions

### Core Verification
- [ ] Handle symlinks correctly
- [ ] Preserve file permissions
- [ ] Handle non-UTF8 filenames
- [ ] Comprehensive error handling
- [ ] Test race conditions
- [ ] Verify parallel execution safety
- [ ] Test with special characters in paths
- [ ] Verify cleanup on failure

### Future Extensions
- [ ] Add support for compression (gzip, zstd)
- [ ] Implement incremental archiving
- [ ] Add file metadata to archive
- [ ] Support for cloud storage backends
- [ ] Add progress reporting
- [ ] Implement archive encryption
- [ ] Add file deduplication
- [ ] Support for archive splitting
